1……..
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_california_housing

# Load data
df = pd.DataFrame(fetch_california_housing().data, columns=fetch_california_housing().feature_names)

# Plot histograms
df.hist(bins=40, figsize=(14, 8), edgecolor='skyblue')
plt.suptitle("Feature Histograms")
plt.show()

# Plot box plots
plt.figure(figsize=(14, 8))
for i, col in enumerate(df.columns, 1):
    plt.subplot(3, 3, i)
    sns.boxplot(y=df[col], color='skyblue')633
    plt.title(col)
plt.tight_layout()
plt.suptitle("Feature Box Plots", y=1.02)
plt.show()

2……………………
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_california_housing

# Load data
data = fetch_california_housing(as_frame=True).frame

# Correlation heatmap
sns.heatmap(data.corr(), annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Correlation Matrix of California Housing')
plt.show()

# Pair plot
sns.pairplot(data, diag_kind='kde', plot_kws={'alpha': 0.5})
plt.suptitle('Pair Plot of California Housing', y=1.02)
plt.show()

3……..
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.decomposition import PCA

# Load data
iris = load_iris()
X, y = iris.data, iris.target

# Reduce to 2D using PCA
X_pca = PCA(n_components=2).fit_transform(X)

# Plot
colors = ['r', 'g', 'b']
for i, label in enumerate(set(y)):
    plt.scatter(X_pca[y == label, 0], X_pca[y == label, 1], 
                color=colors[i], label=iris.target_names[label])

plt.title('PCA on Iris Dataset')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2'')
plt.legend()
plt.grid(True)
plt.show()






4…………….
import pandas as pd


def find_s_algorithm(file_path):
    data = pd.read_csv(file_path)

    print("Training data:")
    print(data)

    attributes = data.columns[:-1]
    class_label = data.columns[-1]

    hypothesis = ['?' for _ in attributes]

    for index, row in data.iterrows():
        if row[class_label] == 'Yes':
            for i, value in enumerate(row[attributes]):
                if hypothesis[i] == '?' or hypothesis[i] == value:
                    hypothesis[i] = value
                else:
                    hypothesis[i] = '?'

    return hypothesis


file_path = 'training_data.csv'
hypothesis = find_s_algorithm(file_path)
print("\nThe final hypothesis is:", hypothesis)


5………….
import numpy as np
import matplotlib.pyplot as plt
from collections import Counter

data = np.random.rand(100)

labels = ["Class1" if x <= 0.5 else "Class2" for x in data[:50]]


def euclidean_distance(x1, x2):
    return abs(x1 - x2)


def knn_classifier(train_data, train_labels, test_point, k):
    distances = [(euclidean_distance(test_point, train_data[i]), train_labels[i]) for i in range(len(train_data))]

    distances.sort(key=lambda x: x[0])
    k_nearest_neighbors = distances[:k]

    k_nearest_labels = [label for _, label in k_nearest_neighbors]

    return Counter(k_nearest_labels).most_common(1)[0][0]


train_data = data[:50]
train_labels = labels

test_data = data[50:]

k_values = [1, 2, 3, 4, 5, 20, 30]

print("--- k-Nearest Neighbors Classification ---")
print("Training dataset: First 50 points labeled based on the rule (x <= 0.5 -> Class1, x > 0.5 -> Class2)")
print("Testing dataset: Remaining 50 points to be classified\n")

results = {}

for k in k_values:
    print(f"Results for k = {k}:")
    classified_labels = [knn_classifier(train_data, train_labels, test_point, k) for test_point in test_data]
    results[k] = classified_labels

    for i, label in enumerate(classified_labels, start=51):
        print(f"Point x{i} (value: {test_data[i - 51]:.4f}) is classified as {label}")
    print("\n")

print("Classification complete.\n")

for k in k_values:
    classified_labels = results[k]
    class1_points = [test_data[i] for i in range(len(test_data)) if classified_labels[i] == "Class1"]
    class2_points = [test_data[i] for i in range(len(test_data)) if classified_labels[i] == "Class2"]

    plt.figure(figsize=(10, 6))
    plt.scatter(train_data, [0] * len(train_data), c=["blue" if label == "Class1" else "red" for label in train_labels],
                label="Training Data", marker="o")
    plt.scatter(class1_points, [1] * len(class1_points), c="blue", label="Class1 (Test)", marker="x")
    plt.scatter(class2_points, [1] * len(class2_points), c="red", label="Class2 (Test)", marker="x")

    plt.title(f"k-NN Classification Results for k = {k}")
    plt.xlabel("Data Points")
    plt.ylabel("Classification Level")
    plt.legend()
    plt.grid(True)
    plt.show()


6……………
import numpy as np
import matplotlib.pyplot as plt

# Gaussian kernel
def kernel(x, xi, tau):
    return np.exp(-np.sum((x - xi) ** 2) / (2 * tau ** 2))

# Locally Weighted Regression
def lwr(x, X, y, tau):
    w = np.diag([kernel(x, xi, tau) for xi in X])
    theta = np.linalg.pinv(X.T @ w @ X) @ X.T @ w @ y
    return x @ theta

# Create data
X = np.linspace(0, 2 * np.pi, 100)
y = np.sin(X) + 0.1 * np.random.randn(100)
X_b = np.c_[np.ones(X.shape), X]

# Test points
X_test = np.linspace(0, 2 * np.pi, 200)
X_test_b = np.c_[np.ones(X_test.shape), X_test]

# Predict
tau = 0.5
y_pred = [lwr(xi, X_b, y, tau) for xi in X_test_b]

# Plot
plt.scatter(X, y, color='red', label='Data')
plt.plot(X_test, y_pred, color='blue', label='LWR')
plt.legend()
plt.show()


7……………..
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.metrics import mean_squared_error, r2_score

# Linear Regression on California Housing
def linear_regression():
    data = fetch_california_housing(as_frame=True)
    X = data.data[["AveRooms"]]
    y = data.target

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
    model = LinearRegression().fit(X_train, y_train)
    y_pred = model.predict(X_test)

    plt.scatter(X_test, y_test, color="blue", label="Actual")
    plt.plot(X_test, y_pred, color="red", label="Predicted")
    plt.title("Linear Regression - Housing")
    plt.xlabel("AveRooms")
    plt.ylabel("Target")
    plt.legend()
    plt.show()

    print("Linear Regression:\nMSE =", mean_squared_error(y_test, y_pred))
    print("R² =", r2_score(y_test, y_pred))

# Polynomial Regression on Auto MPG
def polynomial_regression():
    url = "https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data"
    cols = ["mpg", "cyl", "disp", "hp", "weight", "acc", "year", "origin"]
    data = pd.read_csv(url, delim_whitespace=True, names=cols, na_values="?").dropna()

    X = data[["disp"]]
    y = data["mpg"]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

    model = make_pipeline(PolynomialFeatures(2), StandardScaler(), LinearRegression())
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    plt.scatter(X_test, y_test, color="blue", label="Actual")
    plt.scatter(X_test, y_pred, color="red", label="Predicted", s=10)
    plt.title("Polynomial Regression - Auto MPG")
    plt.xlabel("Displacement")
    plt.ylabel("MPG")
    plt.legend()
    plt.show()

    print("Polynomial Regression:\nMSE =", mean_squared_error(y_test, y_pred))
    print("R² =", r2_score(y_test, y_pred))

# Run both
linear_regression()
polynomial_regression()

8…………….
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import accuracy_score

# Load data
X, y = load_breast_cancer(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train model
model = DecisionTreeClassifier(random_state=42)
model.fit(X_train, y_train)

# Predict and evaluate
y_pred = model.predict(X_test)
print(f"Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%")

# Predict a sample
sample = X_test[0].reshape(1, -1)
result = model.predict(sample)[0]
print("Prediction:", "Benign" if result == 1 else "Malignant")

# Visualize tree
plt.figure(figsize=(10, 6))
plot_tree(model, filled=True)
plt.title("Decision Tree - Breast Cancer")
plt.show()



9………………….
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_olivetti_faces
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score

# Load dataset
faces = fetch_olivetti_faces(shuffle=True, random_state=42)
X, y = faces.data, faces.target

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train and predict
model = GaussianNB()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# Accuracy
print(f"Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%")

# Cross-validation
cv_score = cross_val_score(model, X, y, cv=5).mean()
print(f"Cross-validation Accuracy: {cv_score * 100:.2f}%")

# Plot few results
fig, axes = plt.subplots(3, 5, figsize=(12, 6))
for ax, img, true, pred in zip(axes.ravel(), X_test, y_test, y_pred):
    ax.imshow(img.reshape(64, 64), cmap='gray')
    ax.set_title(f"T:{true} P:{pred}")
    ax.axis('off')
plt.tight_layout()
plt.show()


10……………….
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_breast_cancer
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import confusion_matrix, classification_report

# Load and scale data
data = load_breast_cancer()
X = StandardScaler().fit_transform(data.data)
y = data.target

# KMeans clustering
kmeans = KMeans(n_clusters=2, random_state=42)
y_kmeans = kmeans.fit_predict(X)

# Evaluation
print("Confusion Matrix:\n", confusion_matrix(y, y_kmeans))
print("\nClassification Report:\n", classification_report(y, y_kmeans))

# PCA for 2D visualization
X_pca = PCA(n_components=2).fit_transform(X)
centers = PCA(n_components=2).fit(X).transform(kmeans.cluster_centers_)

# Create DataFrame for plotting
df = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])
df['Cluster'] = y_kmeans
df['True'] = y

# Plot clusters, true labels, and centroids
fig, axs = plt.subplots(1, 3, figsize=(18, 5))

sns.scatterplot(data=df, x='PC1', y='PC2', hue='Cluster', ax=axs[0], palette='Set1')
axs[0].set_title('K-Means Clusters')

sns.scatterplot(data=df, x='PC1', y='PC2', hue='True', ax=axs[1], palette='coolwarm')
axs[1].set_title('True Labels')

sns.scatterplot(data=df, x='PC1', y='PC2', hue='Cluster', ax=axs[2], palette='Set1')
axs[2].scatter(centers[:, 0], centers[:, 1], c='red', marker='X', s=200, label='Centroids')
axs[2].legend()
axs[2].set_title('Clusters with Centroids')

plt.tight_layout()
plt.show()
